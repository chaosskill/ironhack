{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps\n",
    "<b>Objective</b>: predict benefits of Healthcare for All obtains through mail donations by “lapsed” doners \n",
    "<br><b>Gather data</b>: data spread across several files that need to be merged \n",
    "<br><b>Clean data</b>: empty cells, weird entries, gender encoding \n",
    "<br><b>Explore</b>: answer questions like: group donors (by gender, by state), what’s the distribution by group, are there noticeable group differences? \n",
    "<br><b>Process</b>: Look at what model to apply (might need to change how some columns are expressed) \n",
    "<br><b>Training</b>: create model to predict donation amount of each doner \n",
    "<br><b>Validate</b>: check predictions against a little subset of the data \n",
    "<br><b>Present</b>: show findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for the dependencies\n",
    "import numpy as np # library for numerical analysis, good for arrays\n",
    "import pandas as pd  # library for data analysis (tabular data) based on numpy\n",
    "import matplotlib.pyplot as plt # library for plotting \n",
    "%matplotlib inline # magic command to display plots in the notebook\n",
    "import seaborn as sns # library for plotting based on matplotlib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('file1.csv') # read the csv file \n",
    "# pd.read_csv('file2.txt', sep='\\t') # read the txt file\n",
    "df2 = pd.read_csv('file2.csv') # read the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head()) # display the first 5 rows\n",
    "#display(df.tail()) # display the last 5 rows\n",
    "print(df.shape) # number of rows and columns\n",
    "#df.columns # column names\n",
    "#len(df.columns) # number of columns\n",
    "df.info() # information about the dataframe\n",
    "df.columns == df2.columns # check if the columns are the same in both dataframes \n",
    "#df.loc[:,[\"gender\",\"id\"]] # show columns by name\n",
    "#df.iloc[:,[2,0]] # show columns by index\n",
    "df['gender'].value_counts(dropna=False) # count the number of values in a column (NaN values are included)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clean data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.lower().str.strip() # convert column names to lower case and strip spaces \n",
    "#df[['state','id']] # show columns by name or reorder columns\n",
    "df = df.reindex(sorted(df.columns), axis=1) # reorder columns by alphabetical order \n",
    "df = df.rename(columns={'id':'ID'}) # rename a column "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df3 = pd.merge(df, df2, on='id') # merge the two dataframes on the column 'id'\n",
    "df3 = pd.concat([df, df2], axis=1) # concatenate the two dataframes on the columns (axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add new columns, filter rows, reset index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3.reset_index(drop=False) # reset the indexes to avoid 2 rows with the same index. Drop=False to keep the old index as a column \n",
    "df3copy = df3.copy() # create a copy of the dataframe (useful for not losing the original dataframe) \n",
    "#df3['new column'] = df3['column1'] + df3['column2'] # create a new column by adding two columns \n",
    "#df3[(df3['target_d']==100) & (df3['very_generous'] == True) ] # filter rows by multiple conditions (AND = \"&\", OR = \"|\", NOT = ~, IN = isin)\n",
    "#df3.T # transpose the dataframe "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix typos, convert to correct data type, deal with empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean values with typos (remove the \"A\" and convert to float)\n",
    "def clean_values(x):\n",
    "    x = str(x)\n",
    "    if ('A' in x ):\n",
    "        x = x.replace('A','')\n",
    "    x = float(x)\n",
    "    return x \n",
    "df3['column'] = df3['column'].apply(clean_values) # apply the function to the column\n",
    "lambda x: float(x.replace(\"A\",\"\")) # same as above but using lambda function (quick and dirty and anonymous function)\n",
    "df3['column'] = df3['column'].map(lambda x: float(x.replace(\"A\",\"\"))) # apply the anon def directly to the column\n",
    "df3['column'] = df3['column'].astype(float) # convert column to float \n",
    "df3['column'] =  pd.to_numeric(df3['column'], errors='coerce') # convert column to float and replace NaN values with 0\n",
    "df3['column'] =  pd.to_datetime(df3['column'], errors='coerce') # convert column to datetime and replace NaN values with 0\n",
    "df3.isna().sum() # count the number of NaN values in each column "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3.drop_duplicates() # remove all duplicates \n",
    "df3 = df3.drop_duplicates(subset=['column1','column2']) # remove duplicates based on two columns \n",
    "df3 = df3.dropna() # drop rows with NaN values\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop missing values\n",
    "This approach is acceptable when we don't know what could be the value of the NA & we are not going to drop a lot of rows (we don't want to end up with a tiny dataframe!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df = pd.DataFrame(round(df3.isna().sum()/len(df3),4)*100) # count the number of NaN values in each column and convert to percentage\n",
    "null_df = null_df.rename(columns = {'index': 'header_name', 0:'percent_nulls'}) # add a column showing null percentages per column \n",
    "columns_drop = null_df[null_df['percent_nulls'] > 90] # get columns with more than 90% of null values \n",
    "df3 = df3.drop(columns_drop.index, axis=1) # drop columns with more than 90% of null values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace missing values\n",
    "Sometimes, we can replace the missing values by a \"reasonable guess\". This \"guess\" can be determined by the domain knowledge (we already know their source and we know what's the value) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['column'] = df3['column'].fillna(df3['column'].mean()) # replace NaN values with the mean of the column (average)\n",
    "df3['column'] = df3['column'].fillna(df3['column'].median()) # replace NaN values with the median of the column (middle value)\n",
    "df3['column'] = df3['column'].fillna(df3['column'].mode()[0]) # replace NaN values with the mode of the column (most frequent value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical check (dispersion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['column'].min() # min & max dispersion \n",
    "df3['column'].std() # standard deviation (how spread out the values are) \n",
    "df3['column'].var() # variance (how spread out the values are, but the values are squared, the higher the variance, the higher the standard deviation) \n",
    "df3['column'].quantile([0.25,0.5,0.75]) # quartiles (25%, 50%, 75%) \n",
    "df3['column'].describe() # summary statistics (count, mean, std, min, 25%, 50%, 75%, max) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # ignore warnings for cleaner output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a histogram \n",
    "df3['column'].plot(kind='hist', bins=20, figsize=(10,5), color='red')\n",
    "plt.title('Histogram of column') \n",
    "plt.xlabel('column') \n",
    "plt.ylabel('Frequency') \n",
    "plt.tight_layout() # adjust the layout to avoid overlapping \n",
    "plt.show() \n",
    "# different approach \n",
    "fig, ax = plt.subplots(figsize=(10,5)) # create a figure and axes\n",
    "ax.hist(df3['column'], bins=20, color='red') # plot a histogram using \n",
    "# using seaborn \n",
    "sns.distplot(df3['column'], bins=20, kde=False, color='red') # plot a histogram \n",
    "plt.show() \n",
    "# interactive plot using plotly \n",
    "import plotly.express as px \n",
    "fig = px.histogram(df3, x='column', nbins=20, title='Histogram of column') \n",
    "fig.show() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.corr(method='pearson') # correlation matrix (pearson, kendall, spearman) \n",
    "# pearson: linear correlation (default), kendall: non-linear correlation, spearman: non-linear correlation (monotonic relationship)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats # import the stats module from scipy \n",
    "round(stats.pearsonr(df3['column1'], df3['column2'])[0],2) # gives the correlation coefficient and the p-value (correlation between two columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
